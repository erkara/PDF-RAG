{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed58a916-f33a-4de3-9f85-8be8888704b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erdi/Documents/temp/temp_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from functools import lru_cache\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "import random\n",
    "\n",
    "\"\"\"Load environment variables and configure device.\"\"\"\n",
    "load_dotenv(\"keys.txt\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(f\"CUDA not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598bdf91-363a-47a0-b1d7-04954c630f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from rag_main import (load_pdfs_from_directory, convert_pdfs_chunks, \n",
    "create_and_save_vector_store, load_vector_store, retrieve_context,\n",
    "load_generation_model, generate_answer, format_references_and_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff92303e-89ff-4831-8379-24a05a943741",
   "metadata": {},
   "source": [
    "# Processing and Chunking PDFs\n",
    "\n",
    "- In this section, our goal is to process all the pdf files and create a vector store that can be used for the downstreaam retrival tasks. To do so, we want to respect the general structure. For example, we need to make sure we dont mix up one document with another during chunking.\n",
    "- Note that the following approach works well if the documents are mostly composed of text since the approach here is adopted for text-based retrival not a multi-model approach. This will be the topic for another project soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65da5cc-a3d6-4e7e-af30-2d46d6ddcea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"cbo_documents/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7573e90a-e9ae-49b0-b33c-95ae73e6476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: 60115-MBR.pdf\n",
      "Loading: 60479-MBR.pdf\n",
      "Loading: 59822_MBR.pdf\n",
      "Loading: 60193-MBR.pdf\n",
      "Loading: 60592-MBR.pdf\n",
      "Loading: 59973-MBR.pdf\n",
      "Loading: 60843-MBR.pdf\n",
      "Loaded 7 PDFs from budgets/\n"
     ]
    }
   ],
   "source": [
    "pdf_documents = load_pdfs_from_directory(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a46435-a302-4afd-9de0-e6c61d5fc836",
   "metadata": {},
   "source": [
    "- *pdf_documents* is a dictory such that keys are the indivual pdf names and values are Langchain \"Document\" objects.\n",
    "- We can make a quick look to check if the loader gets the right content from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c5ac40-2ca6-424c-be85-4bc2712d0439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploded files: dict_keys(['60115-MBR.pdf', '60479-MBR.pdf', '59822_MBR.pdf', '60193-MBR.pdf', '60592-MBR.pdf', '59973-MBR.pdf', '60843-MBR.pdf'])\n",
      "{'source': 'budgets/60115-MBR.pdf', 'page': 0}\n",
      " \n",
      "The amounts shown in this report include the surplus or deficit in the Social Security trust funds\n"
     ]
    }
   ],
   "source": [
    "# check the pdf names\n",
    "print(f\"uploded files: {pdf_documents.keys()}\")\n",
    "\n",
    "# lets see what is in one of those pdfs\n",
    "doc = pdf_documents['60115-MBR.pdf'][0]\n",
    "print(doc.metadata)\n",
    "\n",
    "# we can now look at some of the stuff in this content\n",
    "print(doc.page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a19a3b-7947-4b48-ba5f-64ef13e7f1fd",
   "metadata": {},
   "source": [
    "Now that we have this function, we will convert the extracted text into chunks by making sure that each chunk is tied to its respective file. We will achive this by adding the source information to the metadata of each chunk. Then we will inspect some of the chunks to see if the content is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0f626e-cd0c-47fd-8854-bfc06bc4a8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting pages from 60115-MBR.pdf into chunks...\n",
      "Splitting pages from 60479-MBR.pdf into chunks...\n",
      "Splitting pages from 59822_MBR.pdf into chunks...\n",
      "Splitting pages from 60193-MBR.pdf into chunks...\n",
      "Splitting pages from 60592-MBR.pdf into chunks...\n",
      "Splitting pages from 59973-MBR.pdf into chunks...\n",
      "Splitting pages from 60843-MBR.pdf into chunks...\n",
      "Total chunks created: 312\n"
     ]
    }
   ],
   "source": [
    "chunks = convert_pdfs_chunks(pdf_documents, chunk_size=500, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dbd7140-65ce-45fc-8d9b-534f8c6d26e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_pdf: 60193-MBR.pdf\n",
      "source_page: 6\n",
      "chunk_content:\n",
      "surplus in April. This year, that surplus was $208 billion, CBO estimatesâ€”$32 billion more than \n",
      "the amount recorded last April. Revenues and outlays were higher than they were a year ago. \n",
      "Outlays in April 2023 were lower than they otherwise would have been because certain federal \n",
      "payments due on April 1, 2023, a Saturday, were made in March. If not for that shift, the surplus \n",
      "in April 2024 wou\n"
     ]
    }
   ],
   "source": [
    "# let pick a random chunks and inspect its content\n",
    "random_chunk = random.choice(chunks)\n",
    "print(f\"source_pdf: {random_chunk.metadata['source']}\")\n",
    "print(f\"source_page: {random_chunk.metadata['page']}\")\n",
    "print(f\"chunk_content:\\n{random_chunk.page_content[:400]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0007b0-977f-4e15-b516-c6d0d584caee",
   "metadata": {},
   "source": [
    "# Create Vector Store From Chunks\n",
    "- We are ready to create a vector store. To do that, we need an embedding model that can convert chunks into a high dimensional vector, this is simply another neural network as you can guess. Bottom line is that these models are trained to map similar context to similar vectors(in term of some metric such as cosine similarity). Key point is that it is a good idea to pick a task-spesific embedding model. For example, embedding models such as *FinBERT and FinLang* are primariluty trained on financial documents. We can also use *Sentence-T5 or MiniLM-L6-v2* which are good for general-purpose embeddings. For example, at the bottom of this notebook, you can observe that *FinLang* does a way better job then *Sentence-T5* since we demonstrate the model in financial documents.\n",
    "  \n",
    "- To store our embeddings(vectors plus some metadata), we will use FAISS (Facebook AI Similarity Search) framework. Despite the fact that limited functionality for dynamics updates or RAM usage, it is a good starting point for a small project. The following function will create two files; *index.faiss* which contains the actual vector embeddings and *index.pkl* which has metadata associated with each embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70991b44-aaad-41c9-8040-723d5e1d866d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for chunks...\n",
      "Vector store saved to vectorstore.faiss\n"
     ]
    }
   ],
   "source": [
    "embedding_model_name = 'sentence-transformers/sentence-t5-base'\n",
    "save_path = \"vectorstore.faiss\"\n",
    "vectorstore = create_and_save_vector_store(chunks,embedding_model_name,save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc6eccf-ad86-406b-a09b-f0f8da15e30a",
   "metadata": {},
   "source": [
    "# Retrive Information Based on Query\n",
    "We are now ready to retrive information from our vector store based on our queries. Note that we spesify how many chunks we would like to retrive but did not implement any *ranking logic* within this function. This idea will come up in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84001cd3-f4ad-4772-9b9a-de3e03364c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store from vectorstore.faiss...\n",
      "Querying vector store for: 'What is offical US policy on Pandemics and Biodefense'\n",
      "Chunk 1: MONTHLY BUDGET REVIEW FOR APRIL 2024  MAY 8, 2024 \n",
      "5 \n",
      " Medicaid outlays decreased by $3 billion (or 1 percent) as states continue to reassess the \n",
      "eligibility of enrollees who remained in the program for the duration of the coronavirus \n",
      "public health emergency. (The continuous-enrollment requirement ended on \n",
      "March 31, 2023.) \n",
      "Outlays increased substantially in several other areas: \n",
      " Spending by the Department of Defense (DoD) was $36 billion (or 8 percent) greater than...\n",
      "\n",
      "source information: {'60193-MBR.pdf': [1, [5]], '60115-MBR.pdf': [1, [7]]}\n",
      "==========================================\n",
      "Chunk 2: because in March 2023, the department recorded costs associated with extending the pause \n",
      "on student loan repayments that was instituted during the pandemic. \n",
      " Outlays for Medicaid decreased by $8 billion (or 12 percent). \n",
      " Spending by DoD decreased by $6 billion (or 8 percent). \n",
      " Outlays related to U.S. Coronavirus Refundable Credits decreased by $5 billion (or \n",
      "96 percent). \n",
      " Outlays for international assistance programs decreased by $3 billion (or 70 percent)....\n",
      "\n",
      "source information: {'60193-MBR.pdf': [1, [5]], '60115-MBR.pdf': [1, [7]]}\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "vectorstore = load_vector_store(embedding_model_name)\n",
    "\n",
    "# Step 2: Query the vector store\n",
    "query = \"What is offical US policy on Pandemics and Biodefense\"\n",
    "max_chunks = 2\n",
    "retrieved_chunks, source_info = retrieve_context(vectorstore, query, max_chunks)\n",
    "\n",
    "# Step 3: Display retrieved chunks\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"Chunk {i + 1}: {chunk.page_content[:500]}...\\n\")\n",
    "    # source_pdf:[number_of_chunks, pages]\n",
    "    print(f\"source information: {source_info}\")\n",
    "    print(\"==========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ea3d68-dfc9-4898-8f5d-ba3ec9b6b4fa",
   "metadata": {},
   "source": [
    "# Generation Model\n",
    "\n",
    "- The last step is to spesify which LLM we will use to process the retrived information and give us a an organized final answer. Of course, there are 100s options. We would like to use an open-source one, lets pick *Llama-2-7b-chat*. It is a relatively light-weight model. We will load quantized version of it to speed up the inference.\n",
    "- Note that we have a relative simple logic there to recreate or use the exisiting vector store. We would like to update it when new files are added or the existing ones are modified. In a business setting, this process has to be managed by careful reindexing but we dont need to do that at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97f0fdcb-2c70-4370-b91d-e1d5973b4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation_model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# tokenizer, model = load_generation_model(generation_model)\n",
    "# max_new_tokens = 200\n",
    "# temperature = 0.7\n",
    "\n",
    "# answer = generate_answer(tokenizer, model, retrieved_chunks, query, max_new_tokens,temperature)\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa27e02c-bea5-4890-8fde-3f1bb75c2f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: 60115-MBR.pdf\n",
      "Loading: 60479-MBR.pdf\n",
      "Loading: 59822_MBR.pdf\n",
      "Loading: 60193-MBR.pdf\n",
      "Loading: 60592-MBR.pdf\n",
      "Loading: 59973-MBR.pdf\n",
      "Loading: 60843-MBR.pdf\n",
      "Loaded 7 PDFs from budgets/\n",
      "Splitting pages from 60115-MBR.pdf into chunks...\n",
      "Splitting pages from 60479-MBR.pdf into chunks...\n",
      "Splitting pages from 59822_MBR.pdf into chunks...\n",
      "Splitting pages from 60193-MBR.pdf into chunks...\n",
      "Splitting pages from 60592-MBR.pdf into chunks...\n",
      "Splitting pages from 59973-MBR.pdf into chunks...\n",
      "Splitting pages from 60843-MBR.pdf into chunks...\n",
      "Total chunks created: 312\n",
      "Generating embeddings for chunks...\n",
      "Vector store saved to vectorstore.faiss\n",
      "Loading vector store from vectorstore.faiss...\n",
      "Querying vector store for: 'What was the primary reason for the $309 billion increase in outlays by the Department of Education in fiscal year 2024?'\n",
      "==========================Source Documents/Respective Pages=======================\n",
      " {'60193-MBR.pdf': [6, [7, 2, 6, 5, 4]], '60592-MBR.pdf': [7, [8, 6, 4, 5, 2]], '60843-MBR.pdf': [3, [6]], '60115-MBR.pdf': [3, [7, 2, 5]], '59973-MBR.pdf': [1, [4]]}\n",
      "Loading generation model: meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================GENERATED ANSWER===================\n",
      "The primary reason for the $309 billion increase in outlays by the Department of Education in fiscal year 2024 was because of the interest to loan balances in certain circumstances, and increased eligibility for the Public Service Loan Forgiveness program.\n",
      "\n",
      "Reason: According to the passage, the increase in outlays by the Department of Education was primarily due to the interest to loan balances in certain circumstances, and increased eligibility for the Public Service Loan Forgiveness program. This is evident from the fact that the passage states that \"no modifications have been recorded in the first seven months of fiscal year 2024\" regarding the Department of Education's outlays, indicating that the increase in outlays is due to these two factors.\n",
      "\n",
      "Therefore, the answer to the question is $309 billion.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # simple logic to recreate or use the existing vector database\n",
    "    UPDATE_VS = True\n",
    "    chunk_size = 500\n",
    "    chunk_overlap = 50\n",
    "    data_path = \"budgets/\"\n",
    "    vectorstore_path = \"vectorstore.faiss\"\n",
    "    embedding_model_name = 'FinLang/finance-embeddings-investopedia'\n",
    "    generation_model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "    \n",
    "    max_chunks = 20\n",
    "    max_new_tokens = 300\n",
    "    temperature = 0.7\n",
    "    show_source = True\n",
    "\n",
    "    # User query\n",
    "    query = (\"What was the primary reason for the $309 billion increase in outlays by the Department of Education in fiscal year 2024?\")\n",
    "    \n",
    "    if UPDATE_VS:\n",
    "        pdf_documents = load_pdfs_from_directory(data_path)\n",
    "        chunks = convert_pdfs_chunks(pdf_documents, chunk_size, chunk_overlap)\n",
    "        vectorstore = create_and_save_vector_store(chunks,embedding_model_name,vectorstore_path)\n",
    "        \n",
    "    \n",
    "    # Load vector store\n",
    "    vectorstore = load_vector_store(embedding_model_name,vectorstore_path)\n",
    "\n",
    "    # Retrieve relevant chunks\n",
    "    retrieved_chunks,_ = retrieve_context(vectorstore, query, max_chunks,show_source)\n",
    "\n",
    "    # Load generation model\n",
    "    tokenizer, model = load_generation_model(generation_model)\n",
    "\n",
    "    # Generate an answer\n",
    "    answer = generate_answer(tokenizer, model, retrieved_chunks, query, max_new_tokens,temperature)\n",
    "\n",
    "    # Display the answer\n",
    "    print(\"===================GENERATED ANSWER===================\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f683ba7-bfab-42e3-be50-73339d927258",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "- It is hard to evalute the quaility of a RAG model since it is highly task-dependent. For the purpose of this notebook, we will experiment on the Montly Budget Reviews published by Congreational Budget Office. We developed a set of questions and answer along with where the information can be found. We will use file to see if our RAG pipeline is doing a decent job.\n",
    "- In testing, we will also implement a simple reranking idea. We will count how many chunks is retrived per document based on the user query.\n",
    "Of those chunks, we will retain chunks from the high-score documents. For example; if we have\n",
    "\n",
    "          {'doc1.pdf': 6, 'doc2.pdf': 3, 'doc3.pdf': 1}\n",
    "\n",
    "thresold = total_chunks * 0.2 = 10 * 0.2 = 2. Thus, we will consider the chunks from doc1 and doc2 and filter out doc3. Of course, there are all sort of other ideas to consider as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c6bb0b-5e47-4cce-a261-78563cce1355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "159f94c8-4389-41c4-983b-b05d051bb0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(test_data, vectorstore, tokenizer, model, max_chunks=20, max_new_tokens=400, ratio_to_keep=0.2):\n",
    "    \"\"\"\n",
    "    Evaluate the retrieval and generation pipeline against test cases.\n",
    "    \n",
    "    Args:\n",
    "        test_data (DataFrame): DataFrame containing Question, Answer, Reference, Page columns.\n",
    "        vectorstore (FAISS): The vector store for retrieval.\n",
    "        tokenizer: Tokenizer for the generation model.\n",
    "        model: Generation model.\n",
    "        max_chunks (int): Number of chunks to retrieve.\n",
    "        max_new_tokens (int): Max tokens for generation.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Results with comparison between generated and expected answers.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for _, row in test_data.iterrows():\n",
    "        question = row['Question']\n",
    "        expected_answer = row['Answer']\n",
    "        expected_reference = row['Reference']\n",
    "        expected_page = row['Page']\n",
    "\n",
    "        # Retrieve relevant chunks \n",
    "        retrieved_chunks = vectorstore.similarity_search(question, k=max_chunks)\n",
    "        \n",
    "        # Group chunks by their source metadata\n",
    "        source_counts = Counter(chunk.metadata.get(\"source\", \"unknown\") for chunk in retrieved_chunks)\n",
    "        \n",
    "        # Set a threshold to include multiple relevant sources\n",
    "        threshold = max(1, int(len(retrieved_chunks) * ratio_to_keep))  # At least 20% or at least 1 chunk\n",
    "        relevant_sources = [source for source, count in source_counts.items() if count >= threshold]\n",
    "       \n",
    "        \n",
    "        # Filter chunks to include only those from relevant sources--> is this a good idea?\n",
    "        filtered_chunks = [\n",
    "            chunk for chunk in retrieved_chunks if chunk.metadata.get(\"source\", \"unknown\") in relevant_sources\n",
    "        ]\n",
    "        \n",
    "        # Combine filtered chunks as context, for now top-5 chunks and generate answer\n",
    "        context = \" \".join(chunk.page_content for chunk in filtered_chunks[:5])  # Adjust size if necessary\n",
    "        generated_answer = generate_answer(tokenizer, model, filtered_chunks, question, max_new_tokens)\n",
    "        \n",
    "        # Extract metadata with a fancy helper function\n",
    "        retrieved_references = [chunk.metadata.get(\"source\", \"unknown\") for chunk in filtered_chunks]\n",
    "        retrieved_pages = [chunk.metadata.get(\"page\", \"unknown\") for chunk in filtered_chunks]\n",
    "        formatted_references, formatted_pages = format_references_and_pages(retrieved_references,retrieved_pages)\n",
    "        \n",
    "        print(f\"source information: {dict(source_counts)}\")\n",
    "        print(\"=====================================================\")\n",
    "\n",
    "\n",
    "        # we check if the retrived content contains the ground truth documents\n",
    "        Is_In_Retrieved = \"yes\" if expected_reference in retrieved_references else \"no\"\n",
    "\n",
    "        results.append({\n",
    "            \"Question\": question,\n",
    "            \"Test Answer\": expected_answer,\n",
    "            \"Model Answer\": generated_answer,\n",
    "            \"Test Reference\": expected_reference,\n",
    "            \"Model References\": formatted_references,\n",
    "            \"Test Page\": expected_page,\n",
    "            \"Model Pages\": formatted_pages,\n",
    "            \"Is_In_Retrieved\": Is_In_Retrieved\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0843ffe1-49b5-471b-97e1-7461856ce0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store from vectorstore.faiss...\n",
      "source information: {'60843-MBR.pdf': 11, '60193-MBR.pdf': 1, '60115-MBR.pdf': 2, '60592-MBR.pdf': 3, '59973-MBR.pdf': 2, '59822_MBR.pdf': 1}\n",
      "=====================================================\n",
      "source information: {'60843-MBR.pdf': 7, '60115-MBR.pdf': 3, '59822_MBR.pdf': 2, '60592-MBR.pdf': 2, '59973-MBR.pdf': 3, '60479-MBR.pdf': 2, '60193-MBR.pdf': 1}\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # pdf files and vector store foldr path\n",
    "    data_path = \"budgets/\"\n",
    "    vectorstore_path = \"vectorstore.faiss\"\n",
    "    \n",
    "    # simple logic to recreate or use the existing vector database\n",
    "    UPDATE_VS = False\n",
    "    \n",
    "    # you can play with these as they directly effect the results\n",
    "    chunk_size = 500\n",
    "    chunk_overlap = 50\n",
    "    \n",
    "    # embedding and generation models\n",
    "    embedding_model_name = 'FinLang/finance-embeddings-investopedia'\n",
    "    #embedding_model_name = 'sentence-transformers/sentence-t5-base'\n",
    "    generation_model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "    \n",
    "    # these are about how we manage the retrival and generation process\n",
    "    max_chunks = 20        # max number of chunks retrived\n",
    "    max_new_tokens = 300   # ouput tokens, lower if we need a short precise answer\n",
    "    temperature = 0.7      # lower it if we dont need a creative generation\n",
    "    ratio_to_keep = 0.2    # keep documents contributing to at least 20% chunks \n",
    "\n",
    "    # recrete and load the vector base if needed\n",
    "    if UPDATE_VS:\n",
    "        pdf_documents = load_pdfs_from_directory(data_path)\n",
    "        chunks = convert_pdfs_chunks(pdf_documents, chunk_size, chunk_overlap)\n",
    "        vectorstore = create_and_save_vector_store(chunks,embedding_model_name,vectorstore_path)\n",
    "    vectorstore = load_vector_store(embedding_model_name,vectorstore_path)\n",
    "\n",
    "    # Load generation model\n",
    "    tokenizer, model = load_generation_model(generation_model)\n",
    "\n",
    "    # Generate an answer\n",
    "    test_data = pd.read_excel('cbo_questions.xlsx')\n",
    "\n",
    "    # run evaluation\n",
    "    results_df = evaluate_pipeline(test_data, vectorstore, tokenizer, model,\n",
    "                               max_chunks, max_new_tokens, ratio_to_keep)\n",
    "    results_df.to_csv('evaluation_results.csv',index=False)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d6b8737-5558-40d0-8eeb-a18ad10ac1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Test Answer</th>\n",
       "      <th>Model Answer</th>\n",
       "      <th>Test Reference</th>\n",
       "      <th>Model References</th>\n",
       "      <th>Test Page</th>\n",
       "      <th>Model Pages</th>\n",
       "      <th>Is_In_Retrieved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the percentage of GDP represented by ...</td>\n",
       "      <td>In 2024, individual income tax receipts repres...</td>\n",
       "      <td>In 2024, individual income tax receipts repres...</td>\n",
       "      <td>60843-MBR.pdf</td>\n",
       "      <td>60843-MBR.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>60843-MBR.pdf: [1, 1, 1, 3, 4, 4, 4, 4, 4, 5, 6]</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By how much did receipts from payroll taxes in...</td>\n",
       "      <td>Receipts from payroll taxes increased by $95 b...</td>\n",
       "      <td>According to the text, receipts from payroll t...</td>\n",
       "      <td>60843-MBR.pdf</td>\n",
       "      <td>60843-MBR.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>60843-MBR.pdf: [1, 1, 3, 4, 4, 4, 6]</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  How does the percentage of GDP represented by ...   \n",
       "1  By how much did receipts from payroll taxes in...   \n",
       "\n",
       "                                         Test Answer  \\\n",
       "0  In 2024, individual income tax receipts repres...   \n",
       "1  Receipts from payroll taxes increased by $95 b...   \n",
       "\n",
       "                                        Model Answer Test Reference  \\\n",
       "0  In 2024, individual income tax receipts repres...  60843-MBR.pdf   \n",
       "1  According to the text, receipts from payroll t...  60843-MBR.pdf   \n",
       "\n",
       "  Model References  Test Page  \\\n",
       "0    60843-MBR.pdf          3   \n",
       "1    60843-MBR.pdf          4   \n",
       "\n",
       "                                        Model Pages Is_In_Retrieved  \n",
       "0  60843-MBR.pdf: [1, 1, 1, 3, 4, 4, 4, 4, 4, 5, 6]             yes  \n",
       "1              60843-MBR.pdf: [1, 1, 3, 4, 4, 4, 6]             yes  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a3d81b-7296-48f1-90d6-ee4413a98feb",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- Overally,our rag pipeline is not doing a bad job. We observed that if the documents are mostly composed of text, the pipeline works pretty well. We will soon deploy this pipeline to allow users to upload their own pdfs and communicate with them.\n",
    "- In order to try out the code with a nice user interface, we prepared two different options with Gradio and Streamlit. You can use them as follows:\n",
    "            \n",
    "                                    streamlit run streamlit_UI.py\n",
    "                                    python gradio_UI.py\n",
    "\n",
    "This will promt a message where you can access the portals and interact with code above. Enjoy!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad98f5-4c28-4764-868a-380ee3505f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d80ddb-7daa-4876-a492-7af45773d013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d409e98-8572-4a27-8a52-4e18f0ab941e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
